{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc70daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def get_site_urls():\n",
    "\n",
    "    url = 'https://www.craigslist.org/about/sites'\n",
    "    \n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    \n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "\n",
    "    non_cont_us_str = str(soup).split('\\n<h4>Territories<')[1]\n",
    "\n",
    "    urls = list()\n",
    "\n",
    "    for t in soup.findAll('li'):\n",
    "        if t.find('a'):\n",
    "            s = t.find('a').attrs['href']\n",
    "            if s.endswith('.org') and s not in non_cont_us_str:\n",
    "                urls.append(s)\n",
    "                \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_state_urls(state):\n",
    "    url = f'https://geo.craigslist.org/iso/us/{state}'\n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "\n",
    "    badwords = [\n",
    "        'help',\n",
    "        'safety',\n",
    "        'privacy',\n",
    "        'feedback',\n",
    "        'terms',\n",
    "        'about',\n",
    "        'craigslist app',\n",
    "        'cl is hiring'\n",
    "     ]\n",
    "\n",
    "    urls = list()\n",
    "\n",
    "    for i in [x.find('a') for x in soup.findAll('li')]:\n",
    "        if i and i.string not in badwords:\n",
    "            urls.append(i.attrs['href'])\n",
    "            \n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \n",
    "    url = f'{url}/search/sss?query=pedal+kayak&min_price=&max_price='\n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def conds(t, s, a = 'class'):\n",
    "    return t.attrs and len(t.attrs[a]) > 0 and t.attrs[a][0] == s\n",
    "    \n",
    "def get_raw_data(soup):\n",
    "    \n",
    "    raw_data = list()\n",
    "    for x in soup.findAll('li'):\n",
    "        if conds(x,'result-row'):\n",
    "            raw_data.append(x)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def add_rows(raw_data):\n",
    "    \n",
    "    rows = list()\n",
    "    for row in raw_data:\n",
    "\n",
    "\n",
    "        datetime = row.find('time').attrs['datetime']\n",
    "        \n",
    "        title_str = ''\n",
    "        dollar_str = ''\n",
    "        url = ''\n",
    "        \n",
    "        for x in row.findAll('a'):\n",
    "\n",
    "            if conds(x, 'result-title'):\n",
    "                title_str = x.string\n",
    "\n",
    "            if conds(x, 'result-image') and x.find('span'):\n",
    "\n",
    "                dollar_str = x.find('span').string\n",
    "\n",
    "                url = x.attrs['href']\n",
    "\n",
    "    \n",
    "        \n",
    "        nearby = ''\n",
    "        \n",
    "        for x in row.findAll('span'):\n",
    "            if conds(x, 'nearby'):\n",
    "                nearby = x.attrs['title']\n",
    "\n",
    "\n",
    "        row = {\n",
    "            'datetime': datetime,\n",
    "            'title': title_str,\n",
    "            'amount': dollar_str,\n",
    "            'nearby': nearby,\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "US_URLS = get_site_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01552f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = list()\n",
    "\n",
    "for s in US_URLS:\n",
    "    soup = get_soup(s)\n",
    "    r = add_rows(get_raw_data(soup))\n",
    "    rows += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c53f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=rows)\n",
    "df.to_csv('all_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3daa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
