{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc70daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import SearchEngine\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import math\n",
    "\n",
    "\n",
    "def get_nearby_states(dist_max = 8, homezip = '24016'):\n",
    "    search = SearchEngine()\n",
    "    result = search.by_population(lower=0, upper=1000000000, returns = 1000000000)\n",
    "    \n",
    "    home = search.by_zipcode(homezip)\n",
    "\n",
    "    data = list()\n",
    "    for zipcode in result:\n",
    "        data.append(\n",
    "            {'major_city': zipcode.major_city,\n",
    "             'state': zipcode.state,\n",
    "             'lat': zipcode.lat,\n",
    "             'lon': zipcode.lng,\n",
    "             'dist': math.sqrt((zipcode.lat - home.lat)**2 + (zipcode.lng - home.lng)**2)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    df = pd.DataFrame(data = data) \n",
    "    \n",
    "    zips = df[df['dist'] < dist_max]\n",
    "    \n",
    "    return list(set(zips['state']))\n",
    "\n",
    "    \n",
    "def get_site_urls():\n",
    "\n",
    "    url = 'https://www.craigslist.org/about/sites'\n",
    "    \n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    \n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "\n",
    "    non_cont_us_str = str(soup).split('\\n<h4>Territories<')[1]\n",
    "\n",
    "    urls = list()\n",
    "\n",
    "    for t in soup.findAll('li'):\n",
    "        if t.find('a'):\n",
    "            s = t.find('a').attrs['href']\n",
    "            if s.endswith('.org') and s not in non_cont_us_str:\n",
    "                urls.append(s)\n",
    "                \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_state_urls(state):\n",
    "    url = f'https://geo.craigslist.org/iso/us/{state}'\n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "\n",
    "    badwords = [\n",
    "        'help',\n",
    "        'safety',\n",
    "        'privacy',\n",
    "        'feedback',\n",
    "        'terms',\n",
    "        'about',\n",
    "        'craigslist app',\n",
    "        'cl is hiring'\n",
    "     ]\n",
    "\n",
    "    urls = list()\n",
    "\n",
    "    for i in [x.find('a') for x in soup.findAll('li')]:\n",
    "        if i:\n",
    "            s = i.attrs['href']\n",
    "            if i.string not in badwords and s.endswith('.org'):\n",
    "                if s.startswith('http'):\n",
    "                    urls.append(i.attrs['href'])\n",
    "                else:\n",
    "                    urls.append('http:' + i.attrs['href'])\n",
    "            \n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \n",
    "    url = f'{url}/search/sss?query=pedal+kayak&min_price=&max_price='\n",
    "    headers = {'referer': 'https://usa.fishermap.org/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    r = requests.get(url,headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def conds(t, s, a = 'class'):\n",
    "    return t.attrs and len(t.attrs[a]) > 0 and t.attrs[a][0] == s\n",
    "    \n",
    "def get_raw_data(soup):\n",
    "    \n",
    "    raw_data = list()\n",
    "    for x in soup.findAll('li'):\n",
    "        if conds(x,'result-row'):\n",
    "            raw_data.append(x)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def add_rows(raw_data):\n",
    "    \n",
    "    rows = list()\n",
    "    for row in raw_data:\n",
    "\n",
    "\n",
    "        datetime = row.find('time').attrs['datetime']\n",
    "        \n",
    "        title_str = ''\n",
    "        dollar_str = ''\n",
    "        url = ''\n",
    "        \n",
    "        for x in row.findAll('a'):\n",
    "\n",
    "            if conds(x, 'result-title'):\n",
    "                title_str = x.string\n",
    "\n",
    "            if conds(x, 'result-image') and x.find('span'):\n",
    "\n",
    "                dollar_str = x.find('span').string\n",
    "\n",
    "                url = x.attrs['href']\n",
    "\n",
    "    \n",
    "        \n",
    "        nearby = ''\n",
    "        \n",
    "        for x in row.findAll('span'):\n",
    "            if conds(x, 'nearby'):\n",
    "                nearby = x.attrs['title']\n",
    "\n",
    "\n",
    "        row = {\n",
    "            'datetime': datetime,\n",
    "            'title': title_str,\n",
    "            'amount': dollar_str,\n",
    "            'nearby': nearby,\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "US_URLS = get_site_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = list()\n",
    "\n",
    "# for s in US_URLS:\n",
    "#     soup = get_soup(s)\n",
    "#     r = add_rows(get_raw_data(soup))\n",
    "#     rows += r\n",
    "\n",
    "# df = pd.DataFrame(data=rows)\n",
    "# df.to_csv('all_us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2096f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://charlottesville.craigslist.org\n",
      "https://danville.craigslist.org\n",
      "https://easternshore.craigslist.org\n",
      "https://fredericksburg.craigslist.org\n",
      "https://harrisonburg.craigslist.org\n",
      "https://lynchburg.craigslist.org\n",
      "https://blacksburg.craigslist.org\n",
      "https://norfolk.craigslist.org\n",
      "https://richmond.craigslist.org\n",
      "https://roanoke.craigslist.org\n",
      "https://swva.craigslist.org\n",
      "https://winchester.craigslist.org\n",
      "https://annapolis.craigslist.org\n",
      "https://baltimore.craigslist.org\n",
      "https://chambersburg.craigslist.org\n",
      "https://easternshore.craigslist.org\n",
      "https://frederick.craigslist.org\n",
      "https://smd.craigslist.org\n",
      "https://westmd.craigslist.org\n"
     ]
    }
   ],
   "source": [
    "rows = list()\n",
    "\n",
    "states = ['VA', 'MD']\n",
    "nearby_urls = list()\n",
    "for s in states:\n",
    "    nearby_urls += get_state_urls(s)\n",
    "\n",
    "for s in nearby_urls:\n",
    "    print(s)\n",
    "    try:\n",
    "        soup = get_soup(s)\n",
    "        r = add_rows(get_raw_data(soup))\n",
    "        rows += r\n",
    "    except:\n",
    "        print(f'Failed URL: {s}')\n",
    "        \n",
    "df = pd.DataFrame(data=rows).drop_duplicates()\n",
    "\n",
    "df['amount'] = df['amount'].apply(lambda x: x.replace('$', '').replace(',', ''))\n",
    "df['amount'] = pd.to_numeric(df['amount'])\n",
    "\n",
    "df = df[df['amount'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7a06f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5389e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('close_us.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
